# 理解Transformer

<!--more-->

## 理解自注意力机制
结合up主王木头的讲解[^1]，在经过词向量嵌入操作后，模型输入的每个token都是拥有了基本语义的向量，可以客观表示自身。注意力机制则用来计算每个token与其他token之间的关系，以便在生成输出时考虑上下文信息。  
Attention基本计算公式为：
$$
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
$$
其中，$Q$、$K$、$V$分别表示查询向量都出自于输入的token，$d_k$表示向量的维度。  
在进行计算时，首先对$Q$和$K$进行内积运算，得到了一个查询向量（token）和所有向量（token）的关系矩阵。通过softmax函数将其转化为权重分布，归一化后得到的权重分布表示了每个token对当前token的影响程度。从而得到对应的值向量$V$的加权和，用来计算最终的输出。  
从另一个角度来看，$Q$、$K$、$V$分别表示查询、键、值。用$Q$和$K$计算得到最相关的token，根据相关度与对应的$V$进行加权求和，最终得到当前token的表示。$Q$表示当前正在关注的token，即想找的相关信息；$K$表示其他token的特征，用于显示上下文；$V$表示其他token实际提供的信息。

## 参考资料

[^1]: https://www.bilibili.com/video/BV1XH4y1T76e/ "B站: 王木头讲解"




